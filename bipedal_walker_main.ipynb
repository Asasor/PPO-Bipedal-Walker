{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Based on https://colab.research.google.com/github/turing-club/info/blob/master/BipedalWalker_COGS_workshop.ipynb#scrollTo=DgzIRF71VxNm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from matplotlib import animation, pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Normalizes the inputs\n",
    "class Normalizer():\n",
    "    def __init__(self, nb_inputs):\n",
    "        self.n = np.zeros(nb_inputs)\n",
    "        self.mean = np.zeros(nb_inputs)\n",
    "        self.mean_diff = np.zeros(nb_inputs)\n",
    "        self.var = np.zeros(nb_inputs)\n",
    "\n",
    "    def observe(self, x):\n",
    "        self.n += 1.0\n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (x - self.mean) / self.n\n",
    "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
    "        self.var = (self.mean_diff / self.n).clip(min = 1e-2)\n",
    "\n",
    "    def normalize(self, inputs):\n",
    "        obs_mean = self.mean\n",
    "        obs_std = np.sqrt(self.var)\n",
    "        return (inputs - obs_mean) / obs_std"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "env = gym.make(\"BipedalWalker-v3\")\n",
    "anim_check = False # turn on for saving progress\n",
    "episode_length = 1000\n",
    "nb_steps = 300\n",
    "noise = 0.1\n",
    "learning_rate = 0.2\n",
    "num_deltas = 16\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.shape[0]\n",
    "hidden_size = 4\n",
    "normalizer =  Normalizer(input_size)\n",
    "\n",
    "hidden_layer = np.zeros((hidden_size, input_size))\n",
    "output_layer = np.zeros((output_size, hidden_size))\n",
    "track_record = np.zeros(nb_steps + 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Explore the policy with a given model over one episode\n",
    "def explore(hidden_layer, output_layer):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    num_plays = 0\n",
    "    sum_rewards = 0.0\n",
    "    while not done and num_plays < episode_length:\n",
    "        normalizer.observe(state)\n",
    "        action = output_layer @ (hidden_layer @ normalizer.normalize(state))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward = max(min(reward, 1), -1)\n",
    "        sum_rewards += reward\n",
    "        num_plays += 1\n",
    "    return sum_rewards"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     13\u001B[0m     output_negative \u001B[38;5;241m=\u001B[39m output_layer \u001B[38;5;241m-\u001B[39m delta_output \u001B[38;5;241m*\u001B[39m noise\n\u001B[0;32m     14\u001B[0m     positive_rewards[k] \u001B[38;5;241m=\u001B[39m explore(hidden_positive, output_positive)\n\u001B[1;32m---> 15\u001B[0m     negative_rewards[k] \u001B[38;5;241m=\u001B[39m \u001B[43mexplore\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_negative\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_negative\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# Compute the standard deviation of all rewards\u001B[39;00m\n\u001B[0;32m     18\u001B[0m sigma_rewards \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(positive_rewards \u001B[38;5;241m+\u001B[39m negative_rewards)\u001B[38;5;241m.\u001B[39mstd()\n",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36mexplore\u001B[1;34m(hidden_layer, output_layer)\u001B[0m\n\u001B[0;32m      6\u001B[0m sum_rewards \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done \u001B[38;5;129;01mand\u001B[39;00m num_plays \u001B[38;5;241m<\u001B[39m episode_length:\n\u001B[1;32m----> 8\u001B[0m     \u001B[43mnormalizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobserve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m     action \u001B[38;5;241m=\u001B[39m output_layer \u001B[38;5;241m@\u001B[39m (hidden_layer \u001B[38;5;241m@\u001B[39m normalizer\u001B[38;5;241m.\u001B[39mnormalize(state))\n\u001B[0;32m     10\u001B[0m     state, reward, done, _ \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36mNormalizer.observe\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mobserve\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m\n\u001B[1;32m---> 11\u001B[0m     last_mean \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmean \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (x \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmean) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmean_diff \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (x \u001B[38;5;241m-\u001B[39m last_mean) \u001B[38;5;241m*\u001B[39m (x \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmean)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "init_time = time.time()\n",
    "for step in range(nb_steps + 1):\n",
    "    # initialize the random noise deltas and the positive/negative rewards\n",
    "    positive_rewards, negative_rewards = np.zeros((2, num_deltas))\n",
    "    deltas_hidden = np.random.randn(num_deltas, *hidden_layer.shape)\n",
    "    deltas_output = np.random.randn(num_deltas, *output_layer.shape)\n",
    "\n",
    "    # play an episode each with positive deltas and negative deltas, collect rewards\n",
    "    for k, (delta_hidden, delta_output) in enumerate(zip(deltas_hidden, deltas_output)):\n",
    "        hidden_positive = hidden_layer + delta_hidden * noise\n",
    "        hidden_negative = hidden_layer - delta_hidden * noise\n",
    "        output_positive = output_layer + delta_output * noise\n",
    "        output_negative = output_layer - delta_output * noise\n",
    "        positive_rewards[k] = explore(hidden_positive, output_positive)\n",
    "        negative_rewards[k] = explore(hidden_negative, output_negative)\n",
    "\n",
    "    # Compute the standard deviation of all rewards\n",
    "    sigma_rewards = np.array(positive_rewards + negative_rewards).std()\n",
    "\n",
    "    # compute the advantage of every addition\n",
    "    diffs = positive_rewards - negative_rewards\n",
    "\n",
    "    # Play an episode with the new weights\n",
    "    track_record[step] = explore(hidden_layer, output_layer)\n",
    "\n",
    "    # Update the policy\n",
    "    hidden_derivatives = np.sum((diffs[:, None, None] * deltas_hidden), 0) + np.random.random(hidden_layer.shape) * 0.02\n",
    "    hidden_layer += learning_rate / (num_deltas * sigma_rewards) * hidden_derivatives\n",
    "    output_derivatives = np.sum((diffs[:, None, None] * deltas_output), 0) + np.random.random(output_layer.shape) * 0.02\n",
    "    output_layer += learning_rate / (num_deltas * sigma_rewards) * output_derivatives\n",
    "\n",
    "    # and print the score\n",
    "    print('Step: ', step, 'Reward: ', track_record[step])\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        state = env.reset()\n",
    "        for i in range(500):\n",
    "            env.render(mode='rgb_array')\n",
    "            state, _, done, _ = env.step(output_layer @ (hidden_layer @ normalizer.normalize(state)))\n",
    "            if done:\n",
    "                break\n",
    "        env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# display the robots' walking pattern\n",
    "state = env.reset()\n",
    "for i in range(500):\n",
    "    env.render(mode='rgb_array')\n",
    "    state, _, done, _ = env.step(output_layer @ (hidden_layer @ normalizer.normalize(state)))\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1483.9563500881195\n"
     ]
    }
   ],
   "source": [
    "time_diff = time.time() - init_time\n",
    "print(time_diff)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}